{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call image-to-text DIAL applications\n",
    "\n",
    "[![Jupyter Notebook](./data/images/jupyter-notebook.png)](https://github.com/epam/ai-dial/blob/main/dial-cookbook/examples/how_to_call_image_to_text_applications.ipynb)",
    "\n",
    "\n",
    "From [this notebook](https://github.com/epam/ai-dial/blob/main/dial-cookbook/examples/how_to_call_image_to_text_applications.ipynb), you will learn how to call image-to-text DIAL applications via [DIAL API chat/completions](https://epam-rail.com/dial_api#/paths/~1openai~1deployments~1%7BDeployment%20Name%7D~1chat~1completions/post) call.\n",
    "\n",
    "**DIAL application** is a general term, which encompasses model adapters and application with any custom logic.\n",
    "\n",
    "DIAL currently supports a few image-to-text model adapters:\n",
    "\n",
    "* [GPT4-Vision](https://github.com/epam/ai-dial-adapter-openai/)\n",
    "* [Gemini Pro Vision](https://github.com/epam/ai-dial-adapter-vertexai/)\n",
    "\n",
    "These models follow the same pattern of usage - they take the chat history of interactions between user and model, some of the user messages may contain image attachments, which the model takes into account when it generates the response.\n",
    "\n",
    "The typical use case is to attach an image to a message and ask the model to describe it in the same message.\n",
    "\n",
    "For example purposes, we are going to use a sample `image-size` image-to-text application which returns dimensions of an attached image.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Step 1**: install the necessary dependencies and import the libraries we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests==2.31.0\n",
    "!pip install openai==1.9.0\n",
    "!pip install langchain-openai==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: if DIAL Core server is already configured and running, set env vars `DIAL_URL` and `APP_NAME` to point to the DIAL Core server and the image-to-text application (or model) you want to use.\n",
    "\n",
    "Otherwise, run the [docker-compose file](https://github.com/epam/ai-dial/blob/main/dial-cookbook/docker-compose.yml) in a separate terminal to start the **DIAL Core** server locally along with a sample **image-size** application. The DIAL Core will become available at `http://localhost:8080`:\n",
    "\n",
    "```sh\n",
    "docker compose up core image-size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: configure `DIAL_URL` and `APP_NAME` env vars. The default values are configured under the assumption that DIAL Core is running locally via the docker-compose file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dial_url = os.environ.get(\"DIAL_URL\", \"http://localhost:8080\")\n",
    "os.environ[\"DIAL_URL\"] = dial_url\n",
    "\n",
    "app_name = os.environ.get(\"APP_NAME\", \"image-size\")\n",
    "os.environ[\"APP_NAME\"] = app_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: define helpers to read images from disk and display images in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "\n",
    "def display_base64_image(image_base64):\n",
    "    image_binary = base64.b64decode(image_base64)\n",
    "    display(IPImage(data=image_binary))\n",
    "\n",
    "def read_image_base64(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode()\n",
    "    return image_base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIAL attachments\n",
    "\n",
    "The `render-text` application returns an image in its response.\n",
    "The DIAL API allows to specify a list of attachment files for each message in the DIAL request as well as in the message returned in the DIAL response.\n",
    "\n",
    "The files attached to the request we call **input attachments**. They are saved at the path `messages/{message_idx}/custom_content/attachments/{attachment_idx}`.\n",
    "\n",
    "And the files attached to the response we call **output attachments**. They are saved at the path `message/custom_content/attachments/{attachment_idx}`.\n",
    "\n",
    "A single attachment *(in our case an image attachment)* may either contain the content of the image encoded in base64:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"image/png\",\n",
    "  \"title\": \"Image\",\n",
    "  \"data\": \"<base64-encoded image data>\"\n",
    "}\n",
    "```\n",
    "\n",
    "or reference the attachment content via a URL:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"image/png\",\n",
    "  \"title\": \"Image\",\n",
    "  \"url\": \"<image URL>\"\n",
    "}\n",
    "```\n",
    "\n",
    "The image URL is either\n",
    "1. a publicly accessible URL or\n",
    "2. a URL to an image uploaded to the DIAL Core server beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading file to the DIAL file storage\n",
    "\n",
    "In order to upload an image to the DIAL file storage, we need first to retrieve the user bucket which will be used in all follow-up requests to the DIAL storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: FSWLtFA648cQNf6WfxHZcFzdABKNsTr7ygwQjYbiDi1n\n"
     ]
    }
   ],
   "source": [
    "bucket = requests.get(\n",
    "    f\"{dial_url}/v1/bucket\", headers={\"Api-Key\": \"dial_api_key\"}\n",
    ").json()[\"bucket\"]\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then upload the image to the bucket via multi-part upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {\n",
      "  \"name\": \"square.png\",\n",
      "  \"parentPath\": \"images\",\n",
      "  \"bucket\": \"FSWLtFA648cQNf6WfxHZcFzdABKNsTr7ygwQjYbiDi1n\",\n",
      "  \"url\": \"files/FSWLtFA648cQNf6WfxHZcFzdABKNsTr7ygwQjYbiDi1n/images/square.png\",\n",
      "  \"nodeType\": \"ITEM\",\n",
      "  \"resourceType\": \"FILE\",\n",
      "  \"contentLength\": 1082,\n",
      "  \"contentType\": \"image/png\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/images/square.png\", \"rb\") as file:\n",
    "    metadata = requests.put(\n",
    "        f\"{dial_url}/v1/files/{bucket}/images/square.png\",\n",
    "        headers={\"Api-Key\": \"dial_api_key\"},\n",
    "        files={'file': ('square.png', file, 'image/png')},\n",
    "    ).json()\n",
    "    print(f\"Metadata: {json.dumps(metadata, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image was uploaded to the DIAL storage and now could be accessed by DIAL applications via the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIAL Image URL: files/FSWLtFA648cQNf6WfxHZcFzdABKNsTr7ygwQjYbiDi1n/images/square.png\n"
     ]
    }
   ],
   "source": [
    "dial_image_url = metadata[\"url\"]\n",
    "print(f\"DIAL Image URL: {dial_image_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL is relative to the DIAL Core URL. The application itself will resolve the URL to the full URL by prepending the DIAL Core URL to the relative URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we demonstrate how to call the application via DIAL API using either DIAL storage to save the image or the base64-encoded image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Curl\n",
    "\n",
    "* The application deployment is called `app_name`.\n",
    "* The local DIAL Core server URL is `dial_url`.\n",
    "* The OpenAI API version we are going to use is `2023-12-01-preview`.\n",
    "\n",
    "Therefore, the application is accessible via the URL:\n",
    "\n",
    "```\n",
    "${DIAL_URL}/openai/deployments/${APP_NAME}/chat/completions?api-version=2023-12-01-preview\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using base64-encoded image data\n",
    "\n",
    "The curl command with a singe message with a base64-encoded image is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEsCAIAAABi1XKVAAAEAUlEQVR4nO3WsQ2EQBAEwb8X+ae8JICBt7RUFcFYrTkzP4CE//YAgLcEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsIAMwQIyBAvIECwgQ7CADMECMgQLyBAsIEOwgAzBAjIEC8gQLCBDsICMa3sAH3LObE94NnO2J/AJHhaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAxpnZngDwjocFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQIZgARmCBWQIFpAhWECGYAEZggVkCBaQIVhAhmABGYIFZAgWkCFYQMYNLe4JVeAYhRsAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\"Size: 400x300px\"}}],\"usage\":null,\"id\":\"ec327e92-0393-4a8d-92a0-9f9589c48dd1\",\"created\":1707311404,\"object\":\"chat.completion\"}"
     ]
    }
   ],
   "source": [
    "image_base64 = read_image_base64(\"./data/images/square.png\")\n",
    "\n",
    "os.environ[\"IMAGE_BASE64\"] = image_base64\n",
    "\n",
    "display_base64_image(image_base64)\n",
    "\n",
    "!curl -X POST \"${DIAL_URL}/openai/deployments/${APP_NAME}/chat/completions?api-version=2023-12-01-preview\" \\\n",
    "  -H \"Api-Key:dial_api_key\" \\\n",
    "  -H \"Content-Type:application/json\" \\\n",
    "  -d '{ \"messages\": [ { \"role\": \"user\", \"content\": \"\", \"custom_content\": { \"attachments\": [ { \"type\": \"image/png\", \"data\": \"'\"${IMAGE_BASE64}\"'\" } ] } } ] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DIAL storage\n",
    "\n",
    "Now with the DIAL storage, the request is the same, but instead of `data` field we provide `url` field with the URL to the uploaded image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\"Size: 400x300px\"}}],\"usage\":null,\"id\":\"d3fd9bcb-0331-4e5b-8c14-7ccd7e427b45\",\"created\":1707311413,\"object\":\"chat.completion\"}"
     ]
    }
   ],
   "source": [
    "os.environ[\"IMAGE_URL\"] = dial_image_url\n",
    "\n",
    "!curl -X POST \"${DIAL_URL}/openai/deployments/${APP_NAME}/chat/completions?api-version=2023-12-01-preview\" \\\n",
    "  -H \"Api-Key:dial_api_key\" \\\n",
    "  -H \"Content-Type:application/json\" \\\n",
    "  -d '{ \"messages\": [ { \"role\": \"user\", \"content\": \"\", \"custom_content\": { \"attachments\": [ { \"type\": \"image/png\", \"url\": \"'\"${IMAGE_URL}\"'\" } ] } } ] }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python library Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request from Python using `requests` library.\n",
    "\n",
    "The arguments are identical to the curl command above.\n",
    "\n",
    "From now on, we will demonstrate the DIAL storage use case only. To use the base64-encoded image data, just replace the attachment in the request:\n",
    "\n",
    "```\n",
    "{\"type\": \"image/png\", \"url\": dial_image_url}\n",
    "```\n",
    "\n",
    "with this one:\n",
    "\n",
    "```\n",
    "{\"type\": \"image/png\", \"data\": image_base64}\n",
    "```\n",
    "\n",
    "Let's call the application in the **non-streaming** mode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant', 'content': 'Size: 400x300px'}}],\n",
       " 'usage': None,\n",
       " 'id': '1d01a4fb-93a6-49a4-902d-34d6c523d5e0',\n",
       " 'created': 1707311423,\n",
       " 'object': 'chat.completion'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: 'Size: 400x300px'\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/{app_name}/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"\", \"custom_content\": {\"attachments\": [{\"type\": \"image/png\", \"url\": dial_image_url}]}}]},\n",
    ")\n",
    "body = response.json()\n",
    "display(body)\n",
    "\n",
    "message = body[\"choices\"][0][\"message\"]\n",
    "completion = message[\"content\"]\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Size: 400x300px\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **streaming is enabled**, the chat completion returns a sequence of messages, each containing a chunk of a generated response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\"}}],\"usage\":null,\"id\":\"893f5f25-5138-4b9d-9fe9-588b89c70da7\",\"created\":1707311431,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"Size: 400x300px\"}}],\"usage\":null,\"id\":\"893f5f25-5138-4b9d-9fe9-588b89c70da7\",\"created\":1707311431,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}],\"usage\":null,\"id\":\"893f5f25-5138-4b9d-9fe9-588b89c70da7\",\"created\":1707311431,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: [DONE]'\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/{app_name}/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"\", \"custom_content\": {\"attachments\": [{\"type\": \"image/png\", \"url\": dial_image_url}]}}], \"stream\": True},\n",
    ")\n",
    "for chunk in response.iter_lines():\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python SDK\n",
    "\n",
    "The DIAL deployment could be called using [OpenAI Python SDK](https://pypi.org/project/openai/) as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=app_name,\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='cde9f220-ec58-45de-b924-b470ecd50cef', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Size: 400x300px', role='assistant', function_call=None, tool_calls=None))], created=1707311443, model=None, object='chat.completion', system_fingerprint=None, usage=None)\n",
      "Completion: 'Size: 400x300px'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\",\n",
    "            \"custom_content\": {\"attachments\": [{\"type\": \"image/png\", \"url\": dial_image_url}]}\n",
    "        }\n",
    "    ],\n",
    "    model=app_name,\n",
    ")\n",
    "print(chat_completion)\n",
    "message = chat_completion.choices[0].message\n",
    "completion = message.content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Size: 400x300px\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='29c8af34-1515-4f43-ad90-cab888f67373', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1707311454, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='29c8af34-1515-4f43-ad90-cab888f67373', choices=[Choice(delta=ChoiceDelta(content='Size: 400x300px', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1707311454, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='29c8af34-1515-4f43-ad90-cab888f67373', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1707311454, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "Completion: 'Size: 400x300px'\n"
     ]
    }
   ],
   "source": [
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\",\n",
    "            \"custom_content\": {\"attachments\": [{\"type\": \"image/png\", \"url\": dial_image_url}]}\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    model=app_name,\n",
    ")\n",
    "completion = \"\"\n",
    "for chunk in chat_completion:\n",
    "    print(chunk)\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        completion += content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Size: 400x300px\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain\n",
    "\n",
    "The [LangChain](https://pypi.org/project/langchain-openai/) library **is not suitable** as a client of image-to-text applications, since `langchain-openai<=0.0.2` ignores the additional fields in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = langchain_openai.AzureChatOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=app_name,\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_fields = {\"custom_content\": {\"attachments\": [{\"type\": \"image/png\", \"url\": dial_image_url}]}}\n",
    "\n",
    "try:\n",
    "  llm.generate(messages=[[HumanMessage(content=\"\", additional_kwargs=extra_fields)]])\n",
    "\n",
    "  raise Exception(\"Generation didn't fail\")\n",
    "except Exception as e:\n",
    "  assert str(e) == \"Error code: 422 - {'error': {'message': 'No image attachment was found in the last message', 'type': 'runtime_error'}}\", \"Unexpected error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    output = llm.stream(input=[HumanMessage(content=\"\", additional_kwargs=extra_fields)])\n",
    "    for chunk in output:\n",
    "        print(chunk.dict())\n",
    "\n",
    "    raise Exception(\"Generation didn't fail\")\n",
    "except Exception as e:\n",
    "    assert str(e) == \"Error code: 422 - {'error': {'message': 'No image attachment was found in the last message', 'type': 'runtime_error'}}\", \"Unexpected error\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
