{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call text-to-text DIAL applications\n",
    "\n",
    "From this notebook, you can learn how to call text-to-text DIAL applications via [DIAL API chat/completions](https://epam-rail.com/dial_api#/paths/~1openai~1deployments~1%7BDeployment%20Name%7D~1chat~1completions/post) call.\n",
    "\n",
    "\n",    
    "**[View Jupyter Notebook](https://github.com/epam/ai-dial/blob/main/dial-cookbook/examples/how_to_call_text_to_text_applications.ipynb)**",
    "\n",
    "\n",
    "For this example, we use a sample text-to-text application called **Echo**, which returns the content of the last user message.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Step 1**: install the necessary dependencies and import the libraries we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests==2.31.0\n",
    "!pip install openai==1.9.0\n",
    "!pip install langchain-openai==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: if DIAL Core server is already configured and running, set env vars `DIAL_URL` and `APP_NAME` to point to the DIAL Core server and the text-to-text application (or model) you want to use.\n",
    "\n",
    "Otherwise, run the [docker-compose file](https://github.com/epam/ai-dial/blob/main/dial-cookbook/docker-compose.yml) in a separate terminal to start the **DIAL Core** server locally along with a sample **echo** application. The DIAL Core will become available at `http://localhost:8080`:\n",
    "\n",
    "```sh\n",
    "docker compose up core echo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: configure `DIAL_URL` and `APP_NAME` env vars. The default values are configured under the assumption that DIAL Core is running locally via the docker-compose file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dial_url = os.environ.get(\"DIAL_URL\", \"http://localhost:8080\")\n",
    "os.environ[\"DIAL_URL\"] = dial_url\n",
    "\n",
    "app_name = os.environ.get(\"APP_NAME\", \"echo\")\n",
    "os.environ[\"APP_NAME\"] = app_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Curl\n",
    "\n",
    "* The DIAL deployment is called `app_name`.\n",
    "* The local DIAL Core server URL is `dial_url`.\n",
    "* The OpenAI API version we are going to use is `2023-12-01-preview`.\n",
    "\n",
    "Therefore, the application is accessible via the following URL:\n",
    "\n",
    "```\n",
    "${DIAL_URL}/openai/deployments/${APP_NAME}/chat/completions?api-version=2023-12-01-preview\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curl command that requests completion for a single message chat is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\"Hello world!\"}}],\"usage\":null,\"id\":\"37ffdc98-da4d-48e8-8dec-2d0ec0fd94b1\",\"created\":1707310417,\"object\":\"chat.completion\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"${DIAL_URL}/openai/deployments/${APP_NAME}/chat/completions?api-version=2023-12-01-preview\" \\\n",
    "  -H \"Api-Key:dial_api_key\" \\\n",
    "  -H \"Content-Type:application/json\" \\\n",
    "  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python library Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request using the Python library `requests` and make sure the output message matches the message in the request.\n",
    "\n",
    "The arguments are identical to the curl command above.\n",
    "\n",
    "Let's call the application in the **non-streaming** mode first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant', 'content': 'Hello world!'}}],\n",
       " 'usage': None,\n",
       " 'id': 'dd3647aa-2496-461c-adc4-746e323ee13f',\n",
       " 'created': 1707310430,\n",
       " 'object': 'chat.completion'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/{app_name}/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}]},\n",
    ")\n",
    "body = response.json()\n",
    "display(body)\n",
    "completion = body[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **streaming is enabled**, the chat completion returns a sequence of messages, each containing a chunk of a generated response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\"}}],\"usage\":null,\"id\":\"3c231303-2c25-48a0-bf5e-4e46243ba2eb\",\"created\":1707310448,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"Hello world!\"}}],\"usage\":null,\"id\":\"3c231303-2c25-48a0-bf5e-4e46243ba2eb\",\"created\":1707310448,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}],\"usage\":null,\"id\":\"3c231303-2c25-48a0-bf5e-4e46243ba2eb\",\"created\":1707310448,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: [DONE]'\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/{app_name}/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}], \"stream\": True},\n",
    ")\n",
    "for chunk in response.iter_lines():\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python SDK\n",
    "\n",
    "The DIAL deployment could be called using [OpenAI Python SDK](https://pypi.org/project/openai/) as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=app_name,\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='1d020e70-9de6-402a-a2e0-cb45e340aafa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello world!', role='assistant', function_call=None, tool_calls=None))], created=1707310540, model=None, object='chat.completion', system_fingerprint=None, usage=None)\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world!\",\n",
    "        }\n",
    "    ],\n",
    "    model=app_name,\n",
    ")\n",
    "print(chat_completion)\n",
    "completion = chat_completion.choices[0].message.content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='3a99fb21-d47c-411d-a2c2-6f51ea9d12f6', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1707310529, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3a99fb21-d47c-411d-a2c2-6f51ea9d12f6', choices=[Choice(delta=ChoiceDelta(content='Hello world!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1707310529, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3a99fb21-d47c-411d-a2c2-6f51ea9d12f6', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1707310529, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world!\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    model=app_name,\n",
    ")\n",
    "completion = \"\"\n",
    "for chunk in chat_completion:\n",
    "    print(chunk)\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        completion += content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain\n",
    "\n",
    "Let's call the application via the [LangChain](https://pypi.org/project/langchain-openai/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = langchain_openai.AzureChatOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=app_name,\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='Hello world!', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Hello world!'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('ca6e6bbf-84cb-489a-abcf-9c6ed922713d'))]\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(messages=[[HumanMessage(content=\"Hello world!\")]])\n",
    "print(output)\n",
    "completion = output.generations[0][0].text\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "{'content': 'Hello world!', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "output = llm.stream(input=[HumanMessage(content=\"Hello world!\")])\n",
    "completion = \"\"\n",
    "for chunk in output:\n",
    "    print(chunk.dict())\n",
    "    completion += chunk.content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
