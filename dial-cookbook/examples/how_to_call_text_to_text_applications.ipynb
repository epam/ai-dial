{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call text-to-text DIAL applications\n",
    "\n",
    "From [this notebook](https://github.com/epam/ai-dial/blob/main/dial-cookbook/examples/how_to_call_text_to_text_applications.ipynb), you can learn how to call text-to-text DIAL applications via [DIAL API chat/completions](https://epam-rail.com/dial_api#/paths/~1openai~1deployments~1%7BDeployment%20Name%7D~1chat~1completions/post) call.\n",
    "\n",
    "For this example, we use a sample text-to-text application called **Echo**, which returns the content of the last user message.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Step 1**: install the necessary dependencies and import the libraries we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests==2.31.0\n",
    "!pip install openai==1.9.0\n",
    "!pip install langchain-openai==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: run `docker compose` in a separate terminal to start the **DIAL Core** server locally along with the **Echo** application.\n",
    "\n",
    "```sh\n",
    "(cd ..; docker compose up core echo)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: configure DIAL Core URL: it will be `http://localhost:8080`, if the DIAL Core is run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dial_url = os.environ.get(\"DIAL_URL\", \"http://localhost:8080\")\n",
    "os.environ[\"DIAL_URL\"] = dial_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Curl\n",
    "\n",
    "* The Echo application deployment is called `echo`.\n",
    "* The local DIAL Core server URL is `dial_url`.\n",
    "* The OpenAI API version we are going to use is `2023-12-01-preview`.\n",
    "\n",
    "Therefore, the Echo application is accessible via the following URL:\n",
    "\n",
    "```\n",
    "http://${DIAL_URL}/openai/deployments/echo/chat/completions?api-version=2023-12-01-preview\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding curl command with a singe message in the request is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\"Hello world!\"}}],\"usage\":null,\"id\":\"544c5811-0363-4e45-996a-e03e70972c53\",\"created\":1706540436,\"object\":\"chat.completion\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"${DIAL_URL}/openai/deployments/echo/chat/completions?api-version=2023-12-01-preview\" \\\n",
    "  -H \"Api-Key:dial_api_key\" \\\n",
    "  -H \"Content-Type:application/json\" \\\n",
    "  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python library Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request using the Python library `requests` and make sure the output message matches the message in the request.\n",
    "\n",
    "The arguments are identical to the curl command above.\n",
    "\n",
    "Let's call the application in the **non-streaming** mode first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant', 'content': 'Hello world!'}}],\n",
       " 'usage': None,\n",
       " 'id': '36bece7b-c81b-4415-ad1a-011504bbeed5',\n",
       " 'created': 1706540436,\n",
       " 'object': 'chat.completion'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/echo/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}]},\n",
    ")\n",
    "body = response.json()\n",
    "display(body)\n",
    "completion = body[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **streaming is enabled**, the chat completion returns a sequence of messages, each containing a chunk of a generated response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"role\":\"assistant\"}}],\"usage\":null,\"id\":\"2a75a848-1ba7-427d-b9a7-75cd4b3f6757\",\"created\":1706540436,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":null,\"delta\":{\"content\":\"Hello world!\"}}],\"usage\":null,\"id\":\"2a75a848-1ba7-427d-b9a7-75cd4b3f6757\",\"created\":1706540436,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: {\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{}}],\"usage\":null,\"id\":\"2a75a848-1ba7-427d-b9a7-75cd4b3f6757\",\"created\":1706540436,\"object\":\"chat.completion.chunk\"}'\n",
      "b''\n",
      "b'data: [DONE]'\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{dial_url}/openai/deployments/echo/chat/completions?api-version=2023-12-01-preview\",\n",
    "    headers={\"Api-Key\": \"dial_api_key\"},\n",
    "    json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello world!\"}], \"stream\": True},\n",
    ")\n",
    "for chunk in response.iter_lines():\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python SDK\n",
    "\n",
    "The DIAL deployment could be called using [OpenAI Python SDK](https://pypi.org/project/openai/) as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=\"echo\",\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='17837de1-bdc6-4677-9117-acc4a38bfda9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello world!', role='assistant', function_call=None, tool_calls=None))], created=1706540436, model=None, object='chat.completion', system_fingerprint=None, usage=None)\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world!\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"echo\",\n",
    ")\n",
    "print(chat_completion)\n",
    "completion = chat_completion.choices[0].message.content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='87cfdb6f-64a0-4a4e-9380-f335c915784c', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1706540436, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='87cfdb6f-64a0-4a4e-9380-f335c915784c', choices=[Choice(delta=ChoiceDelta(content='Hello world!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1706540436, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='87cfdb6f-64a0-4a4e-9380-f335c915784c', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1706540436, model=None, object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "chat_completion = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world!\",\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    model=\"echo\",\n",
    ")\n",
    "completion = \"\"\n",
    "for chunk in chat_completion:\n",
    "    print(chunk)\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        completion += content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain\n",
    "\n",
    "Let's call the application via the [LangChain](https://pypi.org/project/langchain-openai/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = langchain_openai.AzureChatOpenAI(\n",
    "    azure_endpoint=dial_url,\n",
    "    azure_deployment=\"echo\",\n",
    "    api_key=\"dial_api_key\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **non-streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='Hello world!', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Hello world!'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('424d3cb8-f89b-4786-9403-ae32f0828eb6'))]\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(messages=[[HumanMessage(content=\"Hello world!\")]])\n",
    "print(output)\n",
    "completion = output.generations[0][0].text\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the application in the **streaming** mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "{'content': 'Hello world!', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {}, 'type': 'AIMessageChunk', 'example': False}\n",
      "Completion: 'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "output = llm.stream(input=[HumanMessage(content=\"Hello world!\")])\n",
    "completion = \"\"\n",
    "for chunk in output:\n",
    "    print(chunk.dict())\n",
    "    completion += chunk.content\n",
    "print(f\"Completion: {completion!r}\")\n",
    "assert completion == \"Hello world!\", \"Unexpected completion\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
