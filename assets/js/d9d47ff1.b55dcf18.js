"use strict";(self.webpackChunkdial=self.webpackChunkdial||[]).push([[6402],{16819:(e,l,n)=>{n.r(l),n.d(l,{assets:()=>r,contentTitle:()=>d,default:()=>c,frontMatter:()=>i,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"tutorials/developers/local-run/quick-start-with-self-hosted-model-ollama","title":"Launch DIAL Chat with a Self-Hosted Model","description":"Introduction","source":"@site/docs/tutorials/1.developers/0.local-run/3.quick-start-with-self-hosted-model-ollama.md","sourceDirName":"tutorials/1.developers/0.local-run","slug":"/tutorials/developers/local-run/quick-start-with-self-hosted-model-ollama","permalink":"/tutorials/developers/local-run/quick-start-with-self-hosted-model-ollama","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"CustomSideBar","previous":{"title":"Chat with OpenAI Model","permalink":"/tutorials/developers/local-run/quick-start-model"},"next":{"title":"Chat with a Self-Hosted Model (vLLM)","permalink":"/tutorials/developers/local-run/quick-start-with-self-hosted-model-vllm"}}');var o=n(74848),s=n(28453);const i={},d="Launch DIAL Chat with a Self-Hosted Model",r={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Get DIAL",id:"step-1-get-dial",level:2},{value:"Step 2: Choose a model to run",id:"step-2-choose-a-model-to-run",level:2},{value:"Chat models",id:"chat-models",level:3},{value:"Vision models",id:"vision-models",level:3},{value:"Embedding models",id:"embedding-models",level:3},{value:"Step 3: Launch DIAL Chat",id:"step-3-launch-dial-chat",level:2}];function h(e){const l={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(l.header,{children:(0,o.jsx)(l.h1,{id:"launch-dial-chat-with-a-self-hosted-model",children:"Launch DIAL Chat with a Self-Hosted Model"})}),"\n",(0,o.jsx)(l.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(l.p,{children:["In this tutorial, you will learn how to quickly launch DIAL Chat with a self-hosted model powered by ",(0,o.jsx)(l.a,{href:"https://ollama.com/",children:"Ollama"}),"."]}),"\n",(0,o.jsxs)(l.blockquote,{children:["\n",(0,o.jsxs)(l.p,{children:["Watch a ",(0,o.jsx)(l.a,{href:"/video%20demos/Developers/Deployment/deploy-ollama",children:"demo video"})," to see it in action."]}),"\n"]}),"\n",(0,o.jsx)(l.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(l.p,{children:"Docker engine installed on your machine (Docker Compose Version 2.20.0 +)."}),"\n",(0,o.jsxs)(l.blockquote,{children:["\n",(0,o.jsxs)(l.p,{children:["Refer to ",(0,o.jsx)(l.a,{href:"https://docs.docker.com/desktop/",children:"Docker"})," documentation."]}),"\n"]}),"\n",(0,o.jsx)(l.h2,{id:"step-1-get-dial",children:"Step 1: Get DIAL"}),"\n",(0,o.jsxs)(l.p,{children:["Clone ",(0,o.jsx)(l.a,{href:"https://github.com/epam/ai-dial/",children:"the repository"})," with the tutorials and change directory to the following folder:"]}),"\n",(0,o.jsx)(l.pre,{children:(0,o.jsx)(l.code,{className:"language-sh",children:"cd dial-docker-compose/ollama\n"})}),"\n",(0,o.jsx)(l.h2,{id:"step-2-choose-a-model-to-run",children:"Step 2: Choose a model to run"}),"\n",(0,o.jsx)(l.p,{children:"Ollama supports a wide range of popular open-source models."}),"\n",(0,o.jsx)(l.p,{children:"Consider first the modality your are interested in - is it a regular text-to-text chat model, a multi-modal vision model or an embedding model?"}),"\n",(0,o.jsxs)(l.p,{children:["Follow the feature tags ",(0,o.jsxs)(l.em,{children:["(",(0,o.jsx)(l.code,{children:"Embeddings"}),", ",(0,o.jsx)(l.code,{children:"Code"}),", ",(0,o.jsx)(l.code,{children:"Tools"}),", ",(0,o.jsx)(l.code,{children:"Vision"}),")"]})," at ",(0,o.jsx)(l.a,{href:"https://ollama.com/search",children:"Ollama Search"})," to find the appropriate model."]}),"\n",(0,o.jsx)(l.p,{children:"We recommend choosing one of the following models which have been tested."}),"\n",(0,o.jsx)(l.h3,{id:"chat-models",children:"Chat models"}),"\n",(0,o.jsxs)(l.table,{children:[(0,o.jsx)(l.thead,{children:(0,o.jsxs)(l.tr,{children:[(0,o.jsx)(l.th,{children:"Model"}),(0,o.jsx)(l.th,{children:"Tools"})]})}),(0,o.jsxs)(l.tbody,{children:[(0,o.jsxs)(l.tr,{children:[(0,o.jsx)(l.td,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/llama3.1:8b-instruct-q4_0",children:"llama3.1:8b-instruct-q4_0"})}),(0,o.jsxs)(l.td,{children:["\u2705 ",(0,o.jsx)(l.em,{children:"(only in non-streaming mode)"})]})]}),(0,o.jsxs)(l.tr,{children:[(0,o.jsx)(l.td,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/mistral:7b-instruct-q4_0",children:"mistral:7b-instruct-q4_0"})}),(0,o.jsx)(l.td,{children:"\u274c"})]}),(0,o.jsxs)(l.tr,{children:[(0,o.jsx)(l.td,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/phi3.5:3.8b-mini-instruct-q4_0",children:"phi3.5:3.8b-mini-instruct-q4_0"})}),(0,o.jsx)(l.td,{children:"\u274c"})]}),(0,o.jsxs)(l.tr,{children:[(0,o.jsx)(l.td,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/gemma2:2b-instruct-q4_0",children:"gemma2:2b-instruct-q4_0"})}),(0,o.jsx)(l.td,{children:"\u274c"})]})]})]}),"\n",(0,o.jsx)(l.p,{children:"All the models support streaming."}),"\n",(0,o.jsx)(l.h3,{id:"vision-models",children:"Vision models"}),"\n",(0,o.jsxs)(l.ul,{children:["\n",(0,o.jsx)(l.li,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/llava:7b-v1.6-mistral-q4_0",children:"llava:7b-v1.6-mistral-q4_0"})}),"\n",(0,o.jsx)(l.li,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/llava-phi3:3.8b-mini-q4_0",children:"llava-phi3:3.8b-mini-q4_0"})}),"\n"]}),"\n",(0,o.jsx)(l.h3,{id:"embedding-models",children:"Embedding models"}),"\n",(0,o.jsxs)(l.ul,{children:["\n",(0,o.jsx)(l.li,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/nomic-embed-text:137m-v1.5-fp16",children:"nomic-embed-text:137m-v1.5-fp16"})}),"\n",(0,o.jsx)(l.li,{children:(0,o.jsx)(l.a,{href:"https://ollama.com/library/bge-m3:567m-fp16",children:"bge-m3:567m-fp16"})}),"\n"]}),"\n",(0,o.jsx)(l.h2,{id:"step-3-launch-dial-chat",children:"Step 3: Launch DIAL Chat"}),"\n",(0,o.jsxs)(l.ol,{children:["\n",(0,o.jsxs)(l.li,{children:["\n",(0,o.jsxs)(l.p,{children:["Configure ",(0,o.jsx)(l.code,{children:".env"})," file in the current directory according to the type of model you've chosen:"]}),"\n",(0,o.jsxs)(l.ul,{children:["\n",(0,o.jsxs)(l.li,{children:["Set ",(0,o.jsx)(l.code,{children:"OLLAMA_CHAT_MODEL"})," for the name of a text model."]}),"\n",(0,o.jsxs)(l.li,{children:["Set ",(0,o.jsx)(l.code,{children:"OLLAMA_VISION_MODEL"})," for the name of a vision model."]}),"\n",(0,o.jsxs)(l.li,{children:["Set ",(0,o.jsx)(l.code,{children:"OLLAMA_EMBEDDING_MODEL"})," for the name of an embedding model."]}),"\n"]}),"\n",(0,o.jsxs)(l.p,{children:[(0,o.jsx)(l.strong,{children:"Note"}),": It's not necessary to configure all the models. If a model isn't set, then it won't be downloaded."]}),"\n"]}),"\n",(0,o.jsxs)(l.li,{children:["\n",(0,o.jsx)(l.p,{children:"Then run the following command to pull and load into the memory of the Ollama server the specified models:"}),"\n",(0,o.jsx)(l.pre,{children:(0,o.jsx)(l.code,{className:"language-sh",children:"docker compose up --abort-on-container-exit\n"})}),"\n",(0,o.jsxs)(l.blockquote,{children:["\n",(0,o.jsxs)(l.p,{children:["Keep in mind that a typical size of a lightweight Ollama model is around a few gigabytes. So it may take a few minutes ",(0,o.jsx)(l.em,{children:"(or more)"})," to download it on the first run, depending on your internet bandwidth and the size of the model you choose."]}),"\n",(0,o.jsxs)(l.p,{children:["The models are fully loaded once ",(0,o.jsx)(l.code,{children:"ollama-setup"})," service prints ",(0,o.jsx)(l.code,{children:"The Ollama server is up and running."})]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(l.li,{children:["\n",(0,o.jsxs)(l.p,{children:["Finally, open ",(0,o.jsx)(l.a,{href:"http://localhost:3000/",children:"http://localhost:3000/"})," in your browser to launch the DIAL Chat application and select an appropriate DIAL deployment to converse with:"]}),"\n",(0,o.jsxs)(l.ul,{children:["\n",(0,o.jsxs)(l.li,{children:["\n",(0,o.jsxs)(l.p,{children:[(0,o.jsx)(l.code,{children:"Self-hosted chat model"})," deployment for the ",(0,o.jsx)(l.code,{children:"OLLAMA_CHAT_MODEL"})]}),"\n"]}),"\n",(0,o.jsxs)(l.li,{children:["\n",(0,o.jsxs)(l.p,{children:[(0,o.jsx)(l.code,{children:"Self-hosted vision model"})," deployment for the ",(0,o.jsx)(l.code,{children:"OLLAMA_VISION_MODEL"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(l.blockquote,{children:["\n",(0,o.jsx)(l.p,{children:"Note, that the vision models we tested, do not support streaming of response. Moreover, they are typically more computationally expensive than the chat models. So it may take minutes for a vision model to respond."}),"\n"]}),"\n",(0,o.jsxs)(l.p,{children:["The embedding model will become available in DIAL under the deployment name ",(0,o.jsx)(l.code,{children:"embedding-model"})," and could be called via the endpoint: ",(0,o.jsx)(l.code,{children:"localhost:8080/openai/deployments/embedding-model/embeddings"}),"."]}),"\n"]}),"\n"]})]})}function c(e={}){const{wrapper:l}={...(0,s.R)(),...e.components};return l?(0,o.jsx)(l,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},28453:(e,l,n)=>{n.d(l,{R:()=>i,x:()=>d});var t=n(96540);const o={},s=t.createContext(o);function i(e){const l=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(l):{...l,...e}}),[l,e])}function d(e){let l;return l=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:l},e.children)}}}]);