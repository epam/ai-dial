"use strict";(self.webpackChunkdial=self.webpackChunkdial||[]).push([[3953],{28453:(e,t,i)=>{i.d(t,{R:()=>n,x:()=>l});var a=i(96540);const o={},s=a.createContext(o);function n(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:n(e.components),a.createElement(s.Provider,{value:t},e.children)}},39910:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>n,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"platform/multimodality","title":"Multimodality","description":"Introduction","source":"@site/docs/platform/5.multimodality.md","sourceDirName":"platform","slug":"/platform/multimodality","permalink":"/platform/multimodality","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"CustomSideBar","previous":{"title":"Admin Panel","permalink":"/platform/admin-panel"},"next":{"title":"Analytics","permalink":"/platform/realtime-analytics-intro"}}');var o=i(74848),s=i(28453);const n={},l="Multimodality",r={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Multimodality in DIAL",id:"multimodality-in-dial",level:2},{value:"Models",id:"models",level:3},{value:"Working with text",id:"working-with-text",level:4},{value:"Working with images",id:"working-with-images",level:4},{value:"Working with audio and video",id:"working-with-audio-and-video",level:4},{value:"Applications",id:"applications",level:3},{value:"Orchestrator",id:"orchestrator",level:3}];function c(e){const t={a:"a",blockquote:"blockquote",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"multimodality",children:"Multimodality"})}),"\n",(0,o.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(t.p,{children:"Multimodality refers to the use of multiple modes or methods to convey information such as text, images, audio, and video, to create a more effective and engaging way of transmitting information."}),"\n",(0,o.jsx)(t.h2,{id:"multimodality-in-dial",children:"Multimodality in DIAL"}),"\n",(0,o.jsxs)(t.p,{children:["DIAL taps into this by connecting to Large Language Models (LLMs) that handle various media types. You can create applications to handle ",(0,o.jsx)(t.a,{href:"/tutorials/developers/apps-development/multimodality/dial-cookbook/examples/how_to_call_image_to_text_applications",children:"specific modality tasks"})," or even ",(0,o.jsx)(t.a,{href:"/video%20demos/Applications/dial-chathub",children:"comprehensive solution (orchestrators)"})," to blend together applications for more complex scenarios."]}),"\n",(0,o.jsxs)(t.p,{children:["Working with different types of media is made available by supporting working with ",(0,o.jsx)(t.a,{href:"https://dialx.ai/dial_api#tag/Files",children:"files"}),". User or application in DIAL, can input and output files that are saved in a dedicated bucket and are accessible based on a flexible permissions model. Files can be provided as an input for multimodal models and generated by them as an output."]}),"\n",(0,o.jsx)(t.h3,{id:"models",children:"Models"}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.a,{href:"/tutorials/user-guide#language-models",children:"DIAL Chat application"})," offers user interface for communication with the ",(0,o.jsx)(t.a,{href:"/platform/supported-models",children:"Supported Models"}),"."]}),"\n",(0,o.jsxs)(t.p,{children:["Connection to LLMs is realized using so-called ",(0,o.jsx)(t.strong,{children:"adapters"}),". Refer to ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-openai",children:"OpenAI"}),", ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-bedrock/?tab=readme-ov-file#supported-models",children:"Bedrock"}),", ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-vertexai/?tab=readme-ov-file#supported-models",children:"Vertex"})," adapters to learn more about them and the supported models. You can use ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-sdk",children:"DIAL SDK"})," to create custom model adapters."]}),"\n",(0,o.jsx)(t.h4,{id:"working-with-text",children:"Working with text"}),"\n",(0,o.jsxs)(t.p,{children:["DIAL has adapters to a variety of ",(0,o.jsx)(t.strong,{children:"text-to-text"})," processing LLMs. Refer to ",(0,o.jsx)(t.a,{href:"/platform/supported-models",children:"Supported Models"})," to view the list of supported models."]}),"\n",(0,o.jsx)(t.h4,{id:"working-with-images",children:"Working with images"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["For ",(0,o.jsx)(t.strong,{children:"image-to-text"})," tasks, DIAL has adapters for GPT-4o, Claude 4 and Gemini 2.5 models."]}),"\n",(0,o.jsxs)(t.li,{children:["For ",(0,o.jsx)(t.strong,{children:"text-to-image"})," tasks, DIAL has adapters for DALL-E-3, Gemini 2.5 Flash Image (Nano Banana), Google Imagen and Stability diffusion models."]}),"\n"]}),"\n",(0,o.jsx)(t.h4,{id:"working-with-audio-and-video",children:"Working with audio and video"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["For ",(0,o.jsx)(t.strong,{children:"audio/video-to-text"})," tasks, DIAL has adapters for Gemini 2.0/2.5. Refer to ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-vertexai/",children:"Vertex Adapter"})," to view all supported models."]}),"\n",(0,o.jsxs)(t.li,{children:["For ",(0,o.jsx)(t.strong,{children:"text-to-video"}),", ",(0,o.jsx)(t.strong,{children:"image-to-video"}),", ",(0,o.jsx)(t.strong,{children:"video-to-video"})," tasks, DIAL supports ",(0,o.jsx)(t.a,{href:"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/video-generation-quickstart",children:"Sora model by OpenAI"}),". Refer to ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-openai/blob/development/README.md#azure-openai-video-api",children:"OpenAI Adapter"})," to learn more."]}),"\n",(0,o.jsxs)(t.li,{children:["For ",(0,o.jsx)(t.strong,{children:"text-to-audio"})," and ",(0,o.jsx)(t.strong,{children:"audio-to-text"})," tasks, ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-adapter-openai?tab=readme-ov-file#azure-audio-api",children:"DIAL OpenAI adapter"})," supports models connected via ",(0,o.jsx)(t.a,{href:"https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard&pivots=azure-openai#audio-api",children:"Azure Audio API"})," such as GPT-4o mini TTS and Whisper."]}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"applications",children:"Applications"}),"\n",(0,o.jsxs)(t.p,{children:["You can use ",(0,o.jsx)(t.a,{href:"https://github.com/epam/ai-dial-sdk",children:"DIAL SDK"})," to create custom applications compatible with the ",(0,o.jsx)(t.a,{href:"/platform/core/about-core#unified-api",children:"DIAL Unified API"}),". Refer to ",(0,o.jsx)(t.a,{href:"/tutorials/developers/local-run/quick-start-with-application",children:"Tutorials"})," to learn how to create a simple application or watch a ",(0,o.jsx)(t.a,{href:"/video%20demos/Developers/Applications/develop-application",children:"demo video"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"Such application can be designed and configured to use multimodal LLMs to perform specific tasks or even form an ecosystem of applications that can interact with each other."}),"\n",(0,o.jsx)(t.p,{children:"In the Cookbook section, you can find several examples:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/tutorials/developers/apps-development/multimodality/dial-cookbook/examples/how_to_call_text_to_text_applications",children:"Text-to-text"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/tutorials/developers/apps-development/multimodality/dial-cookbook/examples/how_to_call_text_to_image_applications",children:"Text-to-image"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/tutorials/developers/apps-development/multimodality/dial-cookbook/examples/how_to_call_image_to_text_applications",children:"Image-to-text"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/tutorials/developers/apps-development/multimodality/dial-cookbook/examples/how_to_call_dalle_3_with_configuration",children:"DALL\xb7E-3"})}),"\n"]}),"\n",(0,o.jsx)(t.h3,{id:"orchestrator",children:"Orchestrator"}),"\n",(0,o.jsxs)(t.p,{children:["Besides creating applications solving specific multimodal tasks, you can create ",(0,o.jsx)(t.a,{href:"/platform/core/apps#quick-apps",children:"orchestrators"})," that can use available AI models as tools to solve a given task in a workflow."]}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.a,{href:"/video%20demos/Applications/dial-chathub",children:"DIAL ChatHub"})," is an example of an orchestrator that combines several applications and models into one unified access point. ChatHub can automatically route prompts to one of several agents (text-to-text applications, text-to-image applications, vision-to-text applications) depending on the task that needs to be performed. For example, if a user asks about weather, the Web RAG agent is engaged, if a user wants to output an image based on the text input - a specific application handles this task that is connected with a corresponding model. All this is done while interacting with one ChatHub solution."]}),"\n",(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsxs)(t.p,{children:["Refer to ",(0,o.jsx)(t.a,{href:"/video%20demos/Applications/quick-apps",children:"Quick Apps 2.0"})," to learn how to create AI agents orchestrators."]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);