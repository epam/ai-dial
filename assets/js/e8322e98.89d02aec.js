"use strict";(self.webpackChunkdial=self.webpackChunkdial||[]).push([[7739],{8467:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>d,toc:()=>a});var l=n(5893),i=n(1151);const o={},s="Launch AI DIAL Chat with a Self-Hosted Model",d={id:"tutorials/quick-start-with-self-hosted-model",title:"Launch AI DIAL Chat with a Self-Hosted Model",description:"Introduction",source:"@site/docs/tutorials/quick-start-with-self-hosted-model.md",sourceDirName:"tutorials",slug:"/tutorials/quick-start-with-self-hosted-model",permalink:"/tutorials/quick-start-with-self-hosted-model",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"CustomSideBar",previous:{title:"Launch AI DIAL Chat with Azure Model",permalink:"/tutorials/quick-start-model"},next:{title:"Launch AI DIAL Chat with a Sample Addon",permalink:"/tutorials/quick-start-with-addon"}},r={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Get AI DIAL",id:"step-1-get-ai-dial",level:2},{value:"Step 2: Choose a model to run",id:"step-2-choose-a-model-to-run",level:2},{value:"Chat models",id:"chat-models",level:3},{value:"Vision models",id:"vision-models",level:3},{value:"Embedding models",id:"embedding-models",level:3},{value:"Step 3: Launch AI DIAL Chat",id:"step-3-launch-ai-dial-chat",level:2}];function h(e){const t={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(t.h1,{id:"launch-ai-dial-chat-with-a-self-hosted-model",children:"Launch AI DIAL Chat with a Self-Hosted Model"}),"\n",(0,l.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,l.jsxs)(t.p,{children:["In this tutorial, you will learn how to quickly launch AI DIAL Chat with a self-hosted model powered by ",(0,l.jsx)(t.a,{href:"https://ollama.com/",children:"Ollama"}),"."]}),"\n",(0,l.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,l.jsx)(t.p,{children:"Docker engine installed on your machine (Docker Compose Version 2.20.0 +)."}),"\n",(0,l.jsxs)(t.blockquote,{children:["\n",(0,l.jsxs)(t.p,{children:["Refer to ",(0,l.jsx)(t.a,{href:"https://docs.docker.com/desktop/",children:"Docker"})," documentation."]}),"\n"]}),"\n",(0,l.jsx)(t.h2,{id:"step-1-get-ai-dial",children:"Step 1: Get AI DIAL"}),"\n",(0,l.jsxs)(t.p,{children:["Clone ",(0,l.jsx)(t.a,{href:"https://github.com/epam/ai-dial/",children:"the repository"})," with the tutorials and change directory to the following folder:"]}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-sh",children:"cd dial-docker-compose/ollama\n"})}),"\n",(0,l.jsx)(t.h2,{id:"step-2-choose-a-model-to-run",children:"Step 2: Choose a model to run"}),"\n",(0,l.jsx)(t.p,{children:"Ollama supports a wide range of popular open-source models."}),"\n",(0,l.jsx)(t.p,{children:"Consider first the modality your are interested in - is it a regular text-to-text chat model, a multi-modal vision model or an embedding model?"}),"\n",(0,l.jsxs)(t.p,{children:["Follow the feature tags ",(0,l.jsxs)(t.em,{children:["(",(0,l.jsx)(t.code,{children:"Embeddings"}),", ",(0,l.jsx)(t.code,{children:"Code"}),", ",(0,l.jsx)(t.code,{children:"Tools"}),", ",(0,l.jsx)(t.code,{children:"Vision"}),")"]})," at ",(0,l.jsx)(t.a,{href:"https://ollama.com/search",children:"Ollama Search"})," to find the appropriate model."]}),"\n",(0,l.jsx)(t.p,{children:"We recommend choosing one of the following models which have been tested."}),"\n",(0,l.jsx)(t.h3,{id:"chat-models",children:"Chat models"}),"\n",(0,l.jsxs)(t.table,{children:[(0,l.jsx)(t.thead,{children:(0,l.jsxs)(t.tr,{children:[(0,l.jsx)(t.th,{children:"Model"}),(0,l.jsx)(t.th,{children:"Tools"})]})}),(0,l.jsxs)(t.tbody,{children:[(0,l.jsxs)(t.tr,{children:[(0,l.jsx)(t.td,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/llama3.1:8b-instruct-q4_0",children:"llama3.1:8b-instruct-q4_0"})}),(0,l.jsxs)(t.td,{children:["\u2705 ",(0,l.jsx)(t.em,{children:"(only in non-streaming mode)"})]})]}),(0,l.jsxs)(t.tr,{children:[(0,l.jsx)(t.td,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/mistral:7b-instruct-q4_0",children:"mistral:7b-instruct-q4_0"})}),(0,l.jsx)(t.td,{children:"\u274c"})]}),(0,l.jsxs)(t.tr,{children:[(0,l.jsx)(t.td,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/phi3.5:3.8b-mini-instruct-q4_0",children:"phi3.5:3.8b-mini-instruct-q4_0"})}),(0,l.jsx)(t.td,{children:"\u274c"})]}),(0,l.jsxs)(t.tr,{children:[(0,l.jsx)(t.td,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/gemma2:2b-instruct-q4_0",children:"gemma2:2b-instruct-q4_0"})}),(0,l.jsx)(t.td,{children:"\u274c"})]})]})]}),"\n",(0,l.jsx)(t.p,{children:"All the models support streaming."}),"\n",(0,l.jsx)(t.h3,{id:"vision-models",children:"Vision models"}),"\n",(0,l.jsxs)(t.ul,{children:["\n",(0,l.jsx)(t.li,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/llava:7b-v1.6-mistral-q4_0",children:"llava:7b-v1.6-mistral-q4_0"})}),"\n",(0,l.jsx)(t.li,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/llava-phi3:3.8b-mini-q4_0",children:"llava-phi3:3.8b-mini-q4_0"})}),"\n"]}),"\n",(0,l.jsx)(t.h3,{id:"embedding-models",children:"Embedding models"}),"\n",(0,l.jsxs)(t.ul,{children:["\n",(0,l.jsx)(t.li,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/nomic-embed-text:137m-v1.5-fp16",children:"nomic-embed-text:137m-v1.5-fp16"})}),"\n",(0,l.jsx)(t.li,{children:(0,l.jsx)(t.a,{href:"https://ollama.com/library/bge-m3:567m-fp16",children:"bge-m3:567m-fp16"})}),"\n"]}),"\n",(0,l.jsx)(t.h2,{id:"step-3-launch-ai-dial-chat",children:"Step 3: Launch AI DIAL Chat"}),"\n",(0,l.jsxs)(t.ol,{children:["\n",(0,l.jsxs)(t.li,{children:["\n",(0,l.jsxs)(t.p,{children:["Configure ",(0,l.jsx)(t.code,{children:".env"})," file in the current directory according to the type of model you've chosen:"]}),"\n",(0,l.jsxs)(t.ul,{children:["\n",(0,l.jsxs)(t.li,{children:["Set ",(0,l.jsx)(t.code,{children:"OLLAMA_CHAT_MODEL"})," for the name of a text model."]}),"\n",(0,l.jsxs)(t.li,{children:["Set ",(0,l.jsx)(t.code,{children:"OLLAMA_VISION_MODEL"})," for the name of a vision model."]}),"\n",(0,l.jsxs)(t.li,{children:["Set ",(0,l.jsx)(t.code,{children:"OLLAMA_EMBEDDING_MODEL"})," for the name of an embedding model."]}),"\n"]}),"\n",(0,l.jsxs)(t.p,{children:[(0,l.jsx)(t.strong,{children:"Note"}),": It's not necessary to configure all the models. If a model isn't set, then it won't be downloaded."]}),"\n"]}),"\n",(0,l.jsxs)(t.li,{children:["\n",(0,l.jsx)(t.p,{children:"Then run the following command to pull and load into the memory of the Ollama server the specified models:"}),"\n",(0,l.jsx)(t.pre,{children:(0,l.jsx)(t.code,{className:"language-sh",children:"docker compose up --abort-on-container-exit\n"})}),"\n",(0,l.jsxs)(t.blockquote,{children:["\n",(0,l.jsxs)(t.p,{children:["Keep in mind that a typical size of a lightweight Ollama model is around a few gigabytes. So it may take a few minutes ",(0,l.jsx)(t.em,{children:"(or dozens of minutes)"})," to download them on the first run depending on your Internet bandwidth."]}),"\n",(0,l.jsxs)(t.p,{children:["The model is fully loaded once ",(0,l.jsx)(t.code,{children:"ollama-setup"})," service prints ",(0,l.jsx)(t.code,{children:"The Ollama server is up and running."})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(t.li,{children:["\n",(0,l.jsxs)(t.p,{children:["Finally, open ",(0,l.jsx)(t.a,{href:"http://localhost:3000/",children:"http://localhost:3000/"})," in your browser to launch the AI DIAL Chat application and select an appropriate AI DIAL deployments to converse with:"]}),"\n",(0,l.jsxs)(t.ul,{children:["\n",(0,l.jsxs)(t.li,{children:[(0,l.jsx)(t.code,{children:"Self-hosted chat model"})," deployment for the ",(0,l.jsx)(t.code,{children:"OLLAMA_CHAT_MODEL"})]}),"\n",(0,l.jsxs)(t.li,{children:[(0,l.jsx)(t.code,{children:"Self-hosted vision model"})," deployment for the ",(0,l.jsx)(t.code,{children:"OLLAMA_VISION_MODEL"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(t.blockquote,{children:["\n",(0,l.jsx)(t.p,{children:"Note, that the vision models we tested, do not support streaming of response. Moreover, they are typically more computationally expensive than the chat models. So it may take minutes for a vision model to respond."}),"\n"]}),"\n",(0,l.jsxs)(t.p,{children:["The embedding model will become available in AI DIAL under the deployment name ",(0,l.jsx)(t.code,{children:"embedding-model"})," and could be called via the endpoint: ",(0,l.jsx)(t.code,{children:"localhost:8080/openai/deployments/embedding-model/embeddings"}),"."]})]})}function c(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,l.jsx)(t,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>d,a:()=>s});var l=n(7294);const i={},o=l.createContext(i);function s(e){const t=l.useContext(o);return l.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),l.createElement(o.Provider,{value:t},e.children)}}}]);