# Prompt Caching

## About Prompt Caching

Prompt caching *(also known as Context caching)* is used to optimize the performance and reduce the operational costs of interacting with large language models (LLMs).

By storing and reusing previously processed data, prompt caching eliminates the need to reprocess the same information in subsequent requests. This approach is particularly beneficial in scenarios such as multi-turn conversations, complex workflows, or applications requiring repetitive queries, where parts of the request (e.g., system instructions, user prompts, or tool definitions) remain unchanged.

By leveraging cached data, systems can significantly reduce computation time, improve response efficiency, and lower token usage costs, making it an essential feature for scalable and cost-effective AI implementations.

## Prompt Caching in LLM

A typical LLM takes a sequence of tokens as an input and produces a sequence of output tokens.
Each consumed token changes the state of LLM in a predictable way. After all input tokens are consumed, output tokens are generated one-by-one.

This suggests how processing of input tokens could be optimized for sequences on input tokens that share the same prefix:

```
input sequence 1: [tokenA, tokenB, tokenC, tokenD, ...]
input sequence 2: [tokenA, tokenB, tokenC, tokenE, ...]
```

One needs to roll the LLM on the first sequence until the shared prefix ends, save this LLM state and then start from this precomputed state when the second sequence is processed.

The end user of LLM is rarely dealing with streams of input tokens directly, instead a more high-level concept of chat request is more commonly used.
However, the idea of prompt caching translates from tokens to chat requests fairy easily - adjacent parts of the chat request (such as tool definitions and messages) eventually map onto adjacent blocks of input tokens which are fed into the LLM.

Therefore, we can relate prompt caching to a sequence of tool definitions and chat messages in chat completion requests.
The requests starting with the same sequence of tools/messages could be computed more efficiently thanks to the prompt caching.

The major model providers support prompt caching: _([OpenAI](https://platform.openai.com/docs/guides/prompt-caching), [Google](https://ai.google.dev/gemini-api/docs/caching?lang=python), [AWS](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html))_.

However, all of them limit the caching scope to a single endpoint. Meaning that in order to leverage caching, the requests sharing the same prefixes must be sent to the same model deployment. This is what DIAL Core is able to do automatically.

## Prompt Caching in DIAL

[DIAL Core](/docs/platform/3.core/0.about-core.md) uses hashing to redirect chat completion requests with the same prefix to the same upstream endpoint:

* Unique hashes are computed for each prefix of an incoming [chat completion request](https://dialx.ai/dial_api#operation/sendChatCompletionRequest).
* The mapping from the hashes to an upstream endpoint and expiration time is save to Redis cache.
* When next request comes in, DIAL computes the hashes again and looks them up the mapping. When there is a match, it sends the request to the corresponding upstream endpoint.

### Cache-Availability Priority Policy

What if DIAL Core found the matching hash in the Redis cache, send the request to the corresponding upstream, but the request has returned an error?

Should DIAL Core persist in retrying the request to the same endpoint *(which may lead to a cache hit)*, or should it try another endpoint *(which is guaranteed to lead to a cache miss)*?

This default Core's choice is the second one, however, it possible to configure the choice is the [chat completion request](https://dialx.ai/dial_api#operation/sendChatCompletionRequest) header `X-CACHE-POLICY`:

- `availability-priority` (default): Prioritizes service availability over cache hits. If the cache upstream is unavailable, the request is routed to another upstream.
- `cache-priority`: Prioritizes cache hits over availability. Requests are retried on the same upstream.

### Types of Prompt caching

DIAL supports two types of prompt caching: [automatic](#automatic) and [manual](#manual). The following table summarizes pros and cons of these types of caching.

|  | Automatic Caching | Manual Caching |
| :-- | :-- | :-- |
| **User Effort**| Minimal (handled by the system) | High (users must explicitly mark cache breakpoints) |
| **Control** | Limited (system decides breakpoints) | Full (users decide breakpoints) |
| **Use Case** | Multi-turn conversations, general efficiency | Custom workflows, advanced use cases |
| **Dependency on Provider** | Depends on automatic caching support from the provider (e.g., OpenAI) | Works as long as the provider supports caching |
| **Flexibility** | Low (system-driven) | High (user-driven) |


### Automatic

Automatic caching happens without explicit user input. The system determines when and where to create cache entries, making it seamless for the user. This is ideal for multi-turn conversations where only new content needs to be processed, while cached content is reused.

##### Deployment configuration

Enable automatic caching by setting the [deployment feature flag](https://github.com/epam/ai-dial-core?tab=readme-ov-file#dynamic-settings) `autoCachingSupported` to `true`.

> **IMPORTANT**: Ensure the language model supports automatic prompt caching before enabling it. Not all models support this feature.

### Manual

Manual Caching is ideal for scenarios that require a precise control over what parts of a request are cached, especially in complex workflows.

You have full control over where cache breakpoints are placed, making it suitable for highly customized workflows. For example, working with a complex prompt (e.g., a mix of tool definitions, system instructions, and user messages) you can explicitly mark parts of the request for caching to optimize performance.

##### Deployment configuration

Enable manual caching by setting the [deployment feature flag](https://github.com/epam/ai-dial-core?tab=readme-ov-file#dynamic-settings) `cacheSupported` to `true`.

> **IMPORTANT**: Ensure the language model supports prompt caching before enabling it. Not all models support this feature.

##### Cache breakpoint

The manual part of this type of prompt caching consists of manual instrumentation of the parts of chat completion request with cache breakpoints.

DIAL Core will compute hashes only for the request prefixes that end with a cache breakpoint.

Individual tool definitions and chat messages could be marked with cache breakpoints via `custom_fields.cache_breakpoint` field.
Each breakpoint may include an optional `expire_at` field that defines time-to-live of the corresponding cache entry in DIAL Core and on the LLM side.

```json
{
    "tools": [
        {
            "type": "function",
            "name": "query_db",
            "parameters": {},
            "custom_fields": {
                "cache_breakpoint": {}
            }
        }
    ],
    "messages": [
        {
            "role": "system",
            "content": "(long instructions)",
            "custom_fields": {
                "cache_breakpoint": {
                    "expire_at": "2014-10-02T15:01:23Z"
                }
            }
        },
        {
            "role": "user",
            "content": "(query)"
        }
    ]
}
```