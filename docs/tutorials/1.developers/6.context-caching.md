# Context Caching

## About Context Caching

Use context caching to improve efficiency and reduce costs when interacting with large language models (LLMs) by reusing previously processed data (or "cached" data) in subsequent requests.

When enabled, parts of a conversation or request (like tool definitions, system prompts, or user messages) can be saved in cache so that they don't need to be reprocessed by the model every time. This saves both time and money, especially for large conversations or complex interactions.

## Context Caching in DIAL

DIAL uses hashing to identify and manage cache entries:

* Each request is broken down into smaller parts (like tools, system messages, or user messages).
* A hashing algorithm generates unique identifiers (hashes) for these parts.
* These hashes are stored in a Redis cache with metadata like the upstream provider and expiration time (TTL).

When a new request is made, the system checks if any part of it matches an existing cache entry. If a match is found, it uses the cached data instead of reprocessing it.

### Types of Context Caching

DIAL support automatic and manual context caching. These approaches define how cache entries (reusable parts of a request) are created and used. 

Automatic caching is great for general use cases where users want the system to handle caching seamlessly, such as multi-turn conversations. Manual caching is ideal for advanced users who need precise control over what parts of a request are cached, especially in complex workflows.

|  | Automatic Caching | Manual Caching |
| :-- | :-- | :-- |
| User Effort | Minimal (handled by the system) | High (users must explicitly mark cache breakpoints) |
| Control | Limited (system decides breakpoints) | Full (users decide breakpoints) |
| Use Case | Multi-turn conversations, general efficiency | Custom workflows, advanced use cases |
| Dependency on Provider | Depends on automatic caching support from the provider (e.g., OpenAI) | Works as long as the provider supports caching |
| Flexibility | Low (system-driven) | High (user-driven) |

#### Automatic

> **IMPORTANT**: Make sure a language model supports context caching before enabling it. Not all models support this feature.

Automatic caching happens without explicit user input. The system decides when and how to create cache entries, making the process seamless for the user. This is ideal for scenarios like multi-turn conversations, where caching can save costs and improve performance. When a new message is added, only the new content is processed, while the cached content is reused.

##### How to Enable

To enable automatic context caching, DIAL Core dynamic setting must include a flag `autoCachingSupported` set to `true`. 

##### How to Use

When making an initial request to DIAL Core:

1. DIAL Core calculates a `PREFIX_HASH_MAP` for all prefixes in tools and messages and sends a request to a model upstream.
3. When DIAL Core receives a response with headers `X-CACHE-BREAKPOINT-PATH` and `X-CACHE-UPDATE-EXPIRE-AT`, it takes the previously calculated `PREFIX_HASH_MAP` and creates a cache entry in Redis with the prefix hash and a corresponding upstream. We should create cache in Redis, only if Upstream actually created cache on his side. For that purpose, Adapter need to return specific header, to notify Core that cache need to be created/updated.

When making subsequent requests:

1. DIAL Core calculates a `PREFIX_HASH_MAP` for all prefixes in tools and messages.
2. DIAL Core traverses `PREFIX_HASH_MAP` from last to first, to find a match in Redis for a prefix and upstream.
3. DIAL Core sends a request to the model adapter with `X-CACHE-BREAKPOINT-PATH` in the header.

#### Manual

> **IMPORTANT**: Make sure a language model supports context caching before enabling it. Not all models support this feature.

Manual Caching is ideal for scenarios that require a precise control over what parts of a request are cached, especially in complex workflows.

You have full control over where cache breakpoints are placed, making it suitable for highly customized workflows. For example, working with a complex prompt (e.g., a mix of tool definitions, system instructions, and user messages) you can explicitly mark parts of the request for caching to optimize performance.

##### How to Enable

To enable manual context caching, DIAL Core dynamic setting must include a flag `cacheSupported` set to `true`. 

##### How to Use

1. Add a `X-CACHE-POLICY` to the chat completion request header:
    - `availability-priority`: Prioritize service availability over cache hits (default). We commit to prioritize service availability over cache hits. We will try to route his request to same upstream, that will provide caching feature. But in case, when cache upstream in not available, we will fallback to another available upstream, and will create cache there.
    - `cache-priority`: Prioritize cache hits over service availability. We commit to prioritize cache hits over service availability. We will route request with cache flag to the same upstream. Even if cache upstream is not available, we will retry request to the same upstream.
2. Add `custom_fields` with `cache_breakpoint` in both tools and messages elements. DIAL Core calculates a hash for the content up to the breakpoint and checks if it exists in the cache. If a cache entry exists, it is reused. If not, the system creates a new cache entry. This hash acts as a "fingerprint" for the content, allowing the system to recognize and reuse cached data efficiently.

        ```json
        {
            "messages": [
                {
                    "role": "user",
                    "content": "...",
                    "custom_fields": {
                        "cache_breakpoint": {
                            "expire_at": "2014-10-02T15:01:23Z"
                        }
                    }
                }
            ]
        }

        ...

        {
            "tools": [
                {
                    "custom_fields": {
                        "cache_breakpoint": {}
                    }
                }
            ]
        }

        ```

3. DIAL Core scans the request for `cache_breakpoint` markers to identify where caching should occur. Each breakpoint indicates that the content up to that point should be hashed and potentially cached.
4. DIAL Core checks if the hash already exists in the Redis cache.
    - If the hash exists: The system reuses the cached data (e.g., routing the request to the same upstream provider or skipping redundant processing).
    - If the hash does not exist: The system stores the new hash in Redis along with metadata (like the upstream provider and expiration time) for future requests.
5. When a new request is made, the Core calculates the hash for the content again and checks if it matches an existing hash in Redis.If a match is found, the cached data is reused, saving time and cost.
