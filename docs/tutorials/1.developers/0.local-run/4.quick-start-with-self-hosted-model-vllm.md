# Launch DIAL Chat with a Self-Hosted Model

## Introduction

In this tutorial, you will learn how to quickly launch DIAL Chat with a self-hosted model powered by [vLLM](https://docs.vllm.ai/en/stable/).

## Prerequisites

Docker engine installed on your machine (Docker Compose Version 2.20.0 +).

> Refer to [Docker](https://docs.docker.com/desktop/) documentation.

## Step 1: Get DIAL

Clone [the repository](https://github.com/epam/ai-dial/) with the tutorials and change directory to the following folder:

```sh
cd dial-docker-compose/vllm
```

## Step 2: Choose a model to run

vLLM supports a wide range of popular open-source models.
We'll demonstrate how integrate HuggingFace chat model served by vLLM in the DIAL Platform.

## Step 3: Launch DIAL Chat

1. Configure `.env` file in the current directory according to the type of model you've chosen:

    * Set `VLLM_CHAT_MODEL` for the name of a chat model. `Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4` from HuggingFace is a default.

2. Then run the following command to run vLLM server and key DIAL Platform components:

    ```sh
    docker compose up --abort-on-container-exit
    ```

    > Keep in mind that a typical size of a lightweight HuggingFace model is around a few gigabytes. So it may take a few minutes _(or more)_ to download it on the first run, depending on your internet bandwidth and the size of the model you choose.

3. Finally, open http://localhost:3000/ in your browser to launch the DIAL Chat application and select an appropriate DIAL deployment to converse with `Self-hosted chat model` deployment for the `VLLM_CHAT_MODEL`.